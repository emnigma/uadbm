{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6cjIFrxl4xpb"
   },
   "source": [
    "# Unsupervised Anomaly Detection Brain-MRI\n",
    "\n",
    "Jupyter notebook for running all the experiments from our [paper](https://arxiv.org/abs/2004.03271). \n",
    "\n",
    "Hyperparameters may have to be adjusted!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KrKRutoIVzqO"
   },
   "source": [
    "## Preperation\n",
    "\n",
    "Installing tensorflow version 1 manually, since it is not supported anymore in colab. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "B2tANEUeV1FO"
   },
   "outputs": [],
   "source": [
    "# !pip uninstall -y tensorflow \n",
    "# !pip install tensorflow-gpu==1.15\n",
    "# !apt install --allow-change-held-packages libcudnn7=7.6.5.32-1+cuda10.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xlBFG8cb9zRr"
   },
   "source": [
    "### Get Code\n",
    "Clone Code from github.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "BmL1urt8-F1a"
   },
   "outputs": [],
   "source": [
    "# ! git clone https://github.com/StefanDenn3r/Unsupervised_Anomaly_Detection_Brain_MRI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "RRskXHhVQd6x"
   },
   "outputs": [],
   "source": [
    "# cd Unsupervised_Anomaly_Detection_Brain_MRI/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hH8OPkp2XchA"
   },
   "source": [
    "Installing all the requirements. \n",
    "\n",
    "Note: Although Colab might say, that a restart of the runtime is necessary, for us it was not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "YXyxDwvMWFKR"
   },
   "outputs": [],
   "source": [
    "# ! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1FRAwbqsRX_x"
   },
   "source": [
    "## Download Brainweb Dataset\n",
    "\n",
    "Downloading the brainweb dataset from [here](https://brainweb.bic.mni.mcgill.ca/). \n",
    "The authors request optionally to provide your name, institution and email. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "hBFA_7f5cUe0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping content/data/Brainweb/lesions/severe/t2_ai_msles2_1mm_pn0_rf0.mnc.gz download, since it already exists.\n",
      "Skipping conversion of content/data/Brainweb/lesions/severe/t2_ai_msles2_1mm_pn0_rf0.mnc.gz, since content/data/Brainweb/lesions/severe/t2_ai_msles2_1mm_pn0_rf0.nii.gz already exists.\n",
      "Skipping content/data/Brainweb/lesions/severe/t2_ai_msles2_1mm_pn0_rf20.mnc.gz download, since it already exists.\n",
      "Skipping conversion of content/data/Brainweb/lesions/severe/t2_ai_msles2_1mm_pn0_rf20.mnc.gz, since content/data/Brainweb/lesions/severe/t2_ai_msles2_1mm_pn0_rf20.nii.gz already exists.\n",
      "Skipping content/data/Brainweb/lesions/severe/t2_ai_msles2_1mm_pn0_rf40.mnc.gz download, since it already exists.\n",
      "Skipping conversion of content/data/Brainweb/lesions/severe/t2_ai_msles2_1mm_pn0_rf40.mnc.gz, since content/data/Brainweb/lesions/severe/t2_ai_msles2_1mm_pn0_rf40.nii.gz already exists.\n",
      "Skipping content/data/Brainweb/lesions/severe/t2_ai_msles2_1mm_pn1_rf0.mnc.gz download, since it already exists.\n",
      "Skipping conversion of content/data/Brainweb/lesions/severe/t2_ai_msles2_1mm_pn1_rf0.mnc.gz, since content/data/Brainweb/lesions/severe/t2_ai_msles2_1mm_pn1_rf0.nii.gz already exists.\n",
      "Skipping content/data/Brainweb/lesions/severe/t2_ai_msles2_1mm_pn1_rf20.mnc.gz download, since it already exists.\n",
      "Skipping conversion of content/data/Brainweb/lesions/severe/t2_ai_msles2_1mm_pn1_rf20.mnc.gz, since content/data/Brainweb/lesions/severe/t2_ai_msles2_1mm_pn1_rf20.nii.gz already exists.\n",
      "Skipping content/data/Brainweb/lesions/severe/t2_ai_msles2_1mm_pn1_rf40.mnc.gz download, since it already exists.\n",
      "Skipping conversion of content/data/Brainweb/lesions/severe/t2_ai_msles2_1mm_pn1_rf40.mnc.gz, since content/data/Brainweb/lesions/severe/t2_ai_msles2_1mm_pn1_rf40.nii.gz already exists.\n",
      "Skipping content/data/Brainweb/lesions/severe/t2_ai_msles2_1mm_pn3_rf0.mnc.gz download, since it already exists.\n",
      "Skipping conversion of content/data/Brainweb/lesions/severe/t2_ai_msles2_1mm_pn3_rf0.mnc.gz, since content/data/Brainweb/lesions/severe/t2_ai_msles2_1mm_pn3_rf0.nii.gz already exists.\n",
      "Skipping content/data/Brainweb/lesions/severe/t2_ai_msles2_1mm_pn3_rf20.mnc.gz download, since it already exists.\n",
      "Skipping conversion of content/data/Brainweb/lesions/severe/t2_ai_msles2_1mm_pn3_rf20.mnc.gz, since content/data/Brainweb/lesions/severe/t2_ai_msles2_1mm_pn3_rf20.nii.gz already exists.\n",
      "Skipping content/data/Brainweb/lesions/severe/t2_ai_msles2_1mm_pn3_rf40.mnc.gz download, since it already exists.\n",
      "Skipping conversion of content/data/Brainweb/lesions/severe/t2_ai_msles2_1mm_pn3_rf40.mnc.gz, since content/data/Brainweb/lesions/severe/t2_ai_msles2_1mm_pn3_rf40.nii.gz already exists.\n",
      "Skipping content/data/Brainweb/lesions/severe/t2_ai_msles2_1mm_pn5_rf0.mnc.gz download, since it already exists.\n",
      "Skipping conversion of content/data/Brainweb/lesions/severe/t2_ai_msles2_1mm_pn5_rf0.mnc.gz, since content/data/Brainweb/lesions/severe/t2_ai_msles2_1mm_pn5_rf0.nii.gz already exists.\n",
      "Skipping content/data/Brainweb/lesions/severe/t2_ai_msles2_1mm_pn5_rf20.mnc.gz download, since it already exists.\n",
      "Skipping conversion of content/data/Brainweb/lesions/severe/t2_ai_msles2_1mm_pn5_rf20.mnc.gz, since content/data/Brainweb/lesions/severe/t2_ai_msles2_1mm_pn5_rf20.nii.gz already exists.\n",
      "Skipping content/data/Brainweb/lesions/severe/t2_ai_msles2_1mm_pn5_rf40.mnc.gz download, since it already exists.\n",
      "Skipping conversion of content/data/Brainweb/lesions/severe/t2_ai_msles2_1mm_pn5_rf40.mnc.gz, since content/data/Brainweb/lesions/severe/t2_ai_msles2_1mm_pn5_rf40.nii.gz already exists.\n",
      "Skipping content/data/Brainweb/normal/t2_icbm_normal_1mm_pn0_rf0.mnc.gz download, since it already exists.\n",
      "Skipping conversion of content/data/Brainweb/normal/t2_icbm_normal_1mm_pn0_rf0.mnc.gz, since content/data/Brainweb/normal/t2_icbm_normal_1mm_pn0_rf0.nii.gz already exists.\n",
      "Skipping content/data/Brainweb/normal/t2_icbm_normal_1mm_pn0_rf20.mnc.gz download, since it already exists.\n",
      "Skipping conversion of content/data/Brainweb/normal/t2_icbm_normal_1mm_pn0_rf20.mnc.gz, since content/data/Brainweb/normal/t2_icbm_normal_1mm_pn0_rf20.nii.gz already exists.\n",
      "Skipping content/data/Brainweb/normal/t2_icbm_normal_1mm_pn0_rf40.mnc.gz download, since it already exists.\n",
      "Skipping conversion of content/data/Brainweb/normal/t2_icbm_normal_1mm_pn0_rf40.mnc.gz, since content/data/Brainweb/normal/t2_icbm_normal_1mm_pn0_rf40.nii.gz already exists.\n",
      "Skipping content/data/Brainweb/normal/t2_icbm_normal_1mm_pn1_rf0.mnc.gz download, since it already exists.\n",
      "Skipping conversion of content/data/Brainweb/normal/t2_icbm_normal_1mm_pn1_rf0.mnc.gz, since content/data/Brainweb/normal/t2_icbm_normal_1mm_pn1_rf0.nii.gz already exists.\n",
      "Skipping content/data/Brainweb/normal/t2_icbm_normal_1mm_pn1_rf20.mnc.gz download, since it already exists.\n",
      "Skipping conversion of content/data/Brainweb/normal/t2_icbm_normal_1mm_pn1_rf20.mnc.gz, since content/data/Brainweb/normal/t2_icbm_normal_1mm_pn1_rf20.nii.gz already exists.\n",
      "Skipping content/data/Brainweb/normal/t2_icbm_normal_1mm_pn1_rf40.mnc.gz download, since it already exists.\n",
      "Skipping conversion of content/data/Brainweb/normal/t2_icbm_normal_1mm_pn1_rf40.mnc.gz, since content/data/Brainweb/normal/t2_icbm_normal_1mm_pn1_rf40.nii.gz already exists.\n",
      "Skipping content/data/Brainweb/normal/t2_icbm_normal_1mm_pn3_rf0.mnc.gz download, since it already exists.\n",
      "Skipping conversion of content/data/Brainweb/normal/t2_icbm_normal_1mm_pn3_rf0.mnc.gz, since content/data/Brainweb/normal/t2_icbm_normal_1mm_pn3_rf0.nii.gz already exists.\n",
      "Skipping content/data/Brainweb/normal/t2_icbm_normal_1mm_pn3_rf20.mnc.gz download, since it already exists.\n",
      "Skipping conversion of content/data/Brainweb/normal/t2_icbm_normal_1mm_pn3_rf20.mnc.gz, since content/data/Brainweb/normal/t2_icbm_normal_1mm_pn3_rf20.nii.gz already exists.\n",
      "Skipping content/data/Brainweb/normal/t2_icbm_normal_1mm_pn3_rf40.mnc.gz download, since it already exists.\n",
      "Skipping conversion of content/data/Brainweb/normal/t2_icbm_normal_1mm_pn3_rf40.mnc.gz, since content/data/Brainweb/normal/t2_icbm_normal_1mm_pn3_rf40.nii.gz already exists.\n",
      "Skipping content/data/Brainweb/normal/t2_icbm_normal_1mm_pn5_rf0.mnc.gz download, since it already exists.\n",
      "Skipping conversion of content/data/Brainweb/normal/t2_icbm_normal_1mm_pn5_rf0.mnc.gz, since content/data/Brainweb/normal/t2_icbm_normal_1mm_pn5_rf0.nii.gz already exists.\n",
      "Skipping content/data/Brainweb/normal/t2_icbm_normal_1mm_pn5_rf20.mnc.gz download, since it already exists.\n",
      "Skipping conversion of content/data/Brainweb/normal/t2_icbm_normal_1mm_pn5_rf20.mnc.gz, since content/data/Brainweb/normal/t2_icbm_normal_1mm_pn5_rf20.nii.gz already exists.\n",
      "Skipping content/data/Brainweb/normal/t2_icbm_normal_1mm_pn5_rf40.mnc.gz download, since it already exists.\n",
      "Skipping conversion of content/data/Brainweb/normal/t2_icbm_normal_1mm_pn5_rf40.mnc.gz, since content/data/Brainweb/normal/t2_icbm_normal_1mm_pn5_rf40.nii.gz already exists.\n",
      "Skipping content/data/Brainweb/groundtruth/normal.mnc.gz download, since it already exists.\n",
      "Skipping conversion of content/data/Brainweb/groundtruth/normal.mnc.gz, since content/data/Brainweb/groundtruth/normal.nii.gz already exists.\n",
      "Skipping content/data/Brainweb/groundtruth/severe_lesions.mnc.gz download, since it already exists.\n",
      "Skipping conversion of content/data/Brainweb/groundtruth/severe_lesions.mnc.gz, since content/data/Brainweb/groundtruth/severe_lesions.nii.gz already exists.\n"
     ]
    }
   ],
   "source": [
    "from utils.brainweb_download import download_brainweb_dataset\n",
    "from pathlib import Path\n",
    "\n",
    "download_brainweb_dataset(\n",
    "    base_dir=Path('./content/data/Brainweb'),\n",
    "    name=\"\",\n",
    "    institution=\"\",\n",
    "    email=\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o8QsqbYA53MI"
   },
   "source": [
    "## Training\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "m1xgAd-K4Q30"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from utils.default_config_setup import get_config, get_options, get_datasets\n",
    "from trainers.AE import AE\n",
    "from trainers.VAE import VAE\n",
    "from trainers.CE import CE\n",
    "from trainers.ceVAE import ceVAE\n",
    "from trainers.VAE_You import VAE_You\n",
    "from trainers.GMVAE import GMVAE\n",
    "from trainers.GMVAE_spatial import GMVAE_spatial\n",
    "\n",
    "# from trainers.fAnoGAN import fAnoGAN\n",
    "from trainers.ConstrainedAAE import ConstrainedAAE\n",
    "from trainers.ConstrainedAE import ConstrainedAE\n",
    "\n",
    "# from trainers.AnoVAEGAN import AnoVAEGAN\n",
    "# from models import (\n",
    "#     autoencoder,\n",
    "#     variational_autoencoder,\n",
    "#     context_encoder_variational_autoencoder,\n",
    "#     variational_autoencoder_Zimmerer,\n",
    "#     context_encoder_variational_autoencoder,\n",
    "#     context_encoder_variational_autoencoder_Zimmerer,\n",
    "#     gaussian_mixture_variational_autoencoder_You,\n",
    "#     gaussian_mixture_variational_autoencoder_spatial,\n",
    "#     gaussian_mixture_variational_autoencoder,\n",
    "#     fanogan,\n",
    "#     fanogan_schlegl,\n",
    "#     constrained_autoencoder,\n",
    "#     constrained_adversarial_autoencoder,\n",
    "#     constrained_adversarial_autoencoder_Chen,\n",
    "#     anovaegan,\n",
    "# )\n",
    "from models import (\n",
    "    autoencoder,\n",
    "    variational_autoencoder,\n",
    "    context_encoder_variational_autoencoder,\n",
    "    variational_autoencoder_Zimmerer,\n",
    "    context_encoder_variational_autoencoder,\n",
    "    context_encoder_variational_autoencoder_Zimmerer,\n",
    "    gaussian_mixture_variational_autoencoder_You,\n",
    "    gaussian_mixture_variational_autoencoder_spatial,\n",
    "    gaussian_mixture_variational_autoencoder,\n",
    "    fanogan,\n",
    "    # fanogan_schlegl,\n",
    "    constrained_autoencoder,\n",
    "    constrained_adversarial_autoencoder,\n",
    "    # constrained_adversarial_autoencoder_Chen,\n",
    "    anovaegan,\n",
    ")\n",
    "from utils import Evaluation\n",
    "from utils.default_config_setup import get_config, get_options, get_datasets, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xogLARJl_B0K"
   },
   "source": [
    "Set paths to datasets and where to save checkpoints and evaluations.\n",
    "\n",
    "**Note:** You may have to adjust `dataset_root` and `save_dir`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "GgkGf2LO35hI"
   },
   "outputs": [],
   "source": [
    "def get_CONFIG(timestamp=None):\n",
    "  current_time = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "  if timestamp:\n",
    "    current_time=timestamp\n",
    "  dataset_root = \"./content/data\"\n",
    "  save_dir = \"./content/saved\"\n",
    "  CONFIG = {\n",
    "    \"BRAINWEBDIR\": os.path.join(dataset_root, 'Brainweb'),\n",
    "#    \"MSSEG2008DIR\": os.path.join(dataset_root, 'MSSEG2008'),\n",
    "#    \"MSISBI2015DIR\": os.path.join(dataset_root, 'ISBIMSlesionChallenge'),\n",
    "#    \"MSLUBDIR\": os.path.join(dataset_root, 'MSlub'),\n",
    "    \"CHECKPOINTDIR\": os.path.join(save_dir, 'checkpoints', current_time),\n",
    "    \"SAMPLEDIR\": os.path.join(save_dir, 'sample_dir', current_time),\n",
    "  }\n",
    "  return CONFIG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L41jcwBrqkev"
   },
   "source": [
    "### Manual Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1-a9c5c4qaiK"
   },
   "source": [
    "#### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "IBobdPWvXBsl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<trainers.AEMODEL.AEMODEL.Config object at 0x29e8a1850>\n",
      "#Params in Bottleneck: 267536\n",
      "#Params in Encoder: 667456\n",
      "#Params in Decoder: 692257\n",
      "#Params in total: 1627249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-13 08:49:56.931536: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [*] Reading checkpoints...\n",
      " [*] Failed to find a checkpoint\n",
      " [!] Load failed...\n",
      "Epoch (TRAIN): [ 0] [   0/   7] loss: 2838.44775391\n",
      "Epoch (TRAIN): [ 0] [   1/   7] loss: 2978.34716797\n",
      "Epoch (TRAIN): [ 0] [   2/   7] loss: 3029.44433594\n",
      "Epoch (TRAIN): [ 0] [   3/   7] loss: 2845.94531250\n",
      "Epoch (TRAIN): [ 0] [   4/   7] loss: 3053.91186523\n",
      "Epoch (TRAIN): [ 0] [   5/   7] loss: 2872.05908203\n",
      "Epoch (TRAIN): [ 0] [   6/   7] loss: 2737.60375977\n",
      "Epoch (VAL): [ 0] [   0/   2] loss: 2555.56567383\n",
      "Epoch (VAL): [ 0] [   1/   2] loss: 2694.03613281\n",
      "Epoch (TRAIN): [ 1] [   0/   7] loss: 3053.90551758\n",
      "Epoch (TRAIN): [ 1] [   1/   7] loss: 2925.19531250\n",
      "Epoch (TRAIN): [ 1] [   2/   7] loss: 2900.31835938\n",
      "Epoch (TRAIN): [ 1] [   3/   7] loss: 2908.61840820\n",
      "Epoch (TRAIN): [ 1] [   4/   7] loss: 2819.11718750\n",
      "Epoch (TRAIN): [ 1] [   5/   7] loss: 2773.35302734\n",
      "Epoch (TRAIN): [ 1] [   6/   7] loss: 2548.50097656\n",
      "Epoch (VAL): [ 1] [   0/   2] loss: 1970.12231445\n",
      "Epoch (VAL): [ 1] [   1/   2] loss: 1943.79418945\n",
      "Epoch (TRAIN): [ 2] [   0/   7] loss: 2176.58593750\n",
      "Epoch (TRAIN): [ 2] [   1/   7] loss: 2072.55468750\n",
      "Epoch (TRAIN): [ 2] [   2/   7] loss: 2065.90673828\n",
      "Epoch (TRAIN): [ 2] [   3/   7] loss: 1809.76232910\n",
      "Epoch (TRAIN): [ 2] [   4/   7] loss: 1791.21899414\n",
      "Epoch (TRAIN): [ 2] [   5/   7] loss: 1731.63964844\n",
      "Epoch (TRAIN): [ 2] [   6/   7] loss: 1619.99145508\n",
      "Epoch (VAL): [ 2] [   0/   2] loss: 1442.21972656\n",
      "Epoch (VAL): [ 2] [   1/   2] loss: 1444.21435547\n",
      "Epoch (TRAIN): [ 3] [   0/   7] loss: 1617.25927734\n",
      "Epoch (TRAIN): [ 3] [   1/   7] loss: 1583.66027832\n",
      "Epoch (TRAIN): [ 3] [   2/   7] loss: 1549.28442383\n",
      "Epoch (TRAIN): [ 3] [   3/   7] loss: 1525.46484375\n",
      "Epoch (TRAIN): [ 3] [   4/   7] loss: 1506.33435059\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m model \u001b[38;5;241m=\u001b[39m AE(tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mSession(), config, network\u001b[38;5;241m=\u001b[39mautoencoder\u001b[38;5;241m.\u001b[39mautoencoder)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Train it\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatasetHC\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[1;32m     16\u001b[0m Evaluation\u001b[38;5;241m.\u001b[39mevaluate(datasetPC, model, options, description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(datasetHC)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptions[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthreshold\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(options[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumEpochs\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n",
      "File \u001b[0;32m~/Data/Unsupervised_Anomaly_Detection_Brain_MRI/trainers/AE.py:47\u001b[0m, in \u001b[0;36mAE.train\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Go go go!\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(last_epoch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnumEpochs):\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m############\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m# TRAINING #\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m############\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPhase\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRAIN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;66;03m# Increment last_epoch counter and save model\u001b[39;00m\n\u001b[1;32m     50\u001b[0m     last_epoch \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Data/Unsupervised_Anomaly_Detection_Brain_MRI/trainers/AE.py:83\u001b[0m, in \u001b[0;36mAE.process\u001b[0;34m(self, dataset, epoch, phase, optim)\u001b[0m\n\u001b[1;32m     75\u001b[0m     fetches[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m optim\n\u001b[1;32m     77\u001b[0m feed_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx: batch,\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout: phase \u001b[38;5;241m==\u001b[39m Phase\u001b[38;5;241m.\u001b[39mTRAIN,\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout_rate: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdropout_rate\n\u001b[1;32m     81\u001b[0m }\n\u001b[0;32m---> 83\u001b[0m run \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeed_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Print to console\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mphase\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m): [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m2d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m4d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_batches\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m4d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.8f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Data/Unsupervised_Anomaly_Detection_Brain_MRI/.env/lib/python3.11/site-packages/tensorflow/python/client/session.py:972\u001b[0m, in \u001b[0;36mBaseSession.run\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    969\u001b[0m run_metadata_ptr \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_NewBuffer() \u001b[38;5;28;01mif\u001b[39;00m run_metadata \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    971\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 972\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions_ptr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mrun_metadata_ptr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    974\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m run_metadata:\n\u001b[1;32m    975\u001b[0m     proto_data \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "File \u001b[0;32m~/Data/Unsupervised_Anomaly_Detection_Brain_MRI/.env/lib/python3.11/site-packages/tensorflow/python/client/session.py:1215\u001b[0m, in \u001b[0;36mBaseSession._run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1212\u001b[0m \u001b[38;5;66;03m# We only want to really perform the run if fetches or targets are provided,\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;66;03m# or if the call is a partial run that specifies feeds.\u001b[39;00m\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m final_fetches \u001b[38;5;129;01mor\u001b[39;00m final_targets \u001b[38;5;129;01mor\u001b[39;00m (handle \u001b[38;5;129;01mand\u001b[39;00m feed_dict_tensor):\n\u001b[0;32m-> 1215\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_targets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_fetches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1216\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mfeed_dict_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1218\u001b[0m   results \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/Data/Unsupervised_Anomaly_Detection_Brain_MRI/.env/lib/python3.11/site-packages/tensorflow/python/client/session.py:1395\u001b[0m, in \u001b[0;36mBaseSession._do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1392\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_tf_sessionprun(handle, feed_dict, fetch_list)\n\u001b[1;32m   1394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1395\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_run_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1396\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1397\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1398\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_call(_prun_fn, handle, feeds, fetches)\n",
      "File \u001b[0;32m~/Data/Unsupervised_Anomaly_Detection_Brain_MRI/.env/lib/python3.11/site-packages/tensorflow/python/client/session.py:1402\u001b[0m, in \u001b[0;36mBaseSession._do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1400\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m   1401\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1402\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1403\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOpError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1404\u001b[0m     message \u001b[38;5;241m=\u001b[39m compat\u001b[38;5;241m.\u001b[39mas_text(e\u001b[38;5;241m.\u001b[39mmessage)\n",
      "File \u001b[0;32m~/Data/Unsupervised_Anomaly_Detection_Brain_MRI/.env/lib/python3.11/site-packages/tensorflow/python/client/session.py:1385\u001b[0m, in \u001b[0;36mBaseSession._do_run.<locals>._run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_fn\u001b[39m(feed_dict, fetch_list, target_list, options, run_metadata):\n\u001b[1;32m   1383\u001b[0m   \u001b[38;5;66;03m# Ensure any changes to the graph are reflected in the runtime.\u001b[39;00m\n\u001b[1;32m   1384\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extend_graph()\n\u001b[0;32m-> 1385\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_tf_sessionrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetch_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1386\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mtarget_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Data/Unsupervised_Anomaly_Detection_Brain_MRI/.env/lib/python3.11/site-packages/tensorflow/python/client/session.py:1478\u001b[0m, in \u001b[0;36mBaseSession._call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1476\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_tf_sessionrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, options, feed_dict, fetch_list, target_list,\n\u001b[1;32m   1477\u001b[0m                         run_metadata):\n\u001b[0;32m-> 1478\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTF_SessionRun_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1479\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mfetch_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1480\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# tf.reset_default_graph()\n",
    "dataset = Dataset.BRAINWEB\n",
    "options = get_options(batchsize=128, learningrate=0.0001, numEpochs=20, zDim=128, outputWidth=128, outputHeight=128, config=get_CONFIG())\n",
    "options['data']['dir'] = options[\"globals\"][dataset.value]\n",
    "datasetHC, datasetPC = get_datasets(options, dataset)\n",
    "config = get_config(trainer=AE, options=options, optimizer='ADAM', intermediateResolutions=[8, 8], dropout_rate=0.1, dataset=datasetHC)\n",
    "print(config)\n",
    "\n",
    "# Create an instance of the model and train it\n",
    "model = AE(tf.compat.v1.Session(), config, network=autoencoder.autoencoder)\n",
    "\n",
    "# Train it\n",
    "model.train(datasetHC)\n",
    "\n",
    "# Evaluate\n",
    "Evaluation.evaluate(datasetPC, model, options, description=f\"{type(datasetHC).__name__}-{options['threshold']}\", epoch=str(options['train']['numEpochs']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-pIg1uHQufK"
   },
   "source": [
    "**VAE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ia25A9wli8d6"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "dataset = Dataset.BRAINWEB\n",
    "options = get_options(batchsize=128, learningrate=0.0001, numEpochs=20, zDim=128, outputWidth=128, outputHeight=128, config=get_CONFIG())\n",
    "options['data']['dir'] = options[\"globals\"][dataset.value]\n",
    "datasetHC, datasetPC = get_datasets(options, dataset)\n",
    "config = get_config(trainer=VAE, options=options, optimizer='ADAM', intermediateResolutions=[8, 8], dropout_rate=0.1, dataset=datasetHC)\n",
    "\n",
    "# Create an instance of the model and train it\n",
    "model = VAE(tf.compat.v1.Session(), config, network=variational_autoencoder.variational_autoencoder)\n",
    "\n",
    "# Train it\n",
    "model.train(datasetHC)\n",
    "\n",
    "# Evaluate\n",
    "Evaluation.evaluate(datasetPC, model, options, description=f\"{type(datasetHC).__name__}-{options['threshold']}\", epoch=str(options['train']['numEpochs']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P2WmjqCqp3S4"
   },
   "source": [
    "#### ceVAE - Variations\n",
    "\n",
    "Paper: [Context-encoding Variational Autoencoder for Unsupervised Anomaly Detection](https://arxiv.org/abs/1812.05941)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AsxKL2xIXhrL"
   },
   "source": [
    "**CE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ed4PbNOc2P50"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "dataset = Dataset.BRAINWEB\n",
    "options = get_options(batchsize=128, learningrate=0.0001, numEpochs=20, zDim=128, outputWidth=128, outputHeight=128, config=get_CONFIG())\n",
    "options['data']['dir'] = options[\"globals\"][dataset.value]\n",
    "datasetHC, datasetPC = get_datasets(options, dataset)\n",
    "config = get_config(trainer=CE, options=options, optimizer='ADAM', intermediateResolutions=[8, 8], dropout_rate=0.1, dataset=datasetHC)\n",
    "\n",
    "# Create an instance of the model and train it\n",
    "model = CE(tf.compat.v1.Session(), config, network=autoencoder.autoencoder)\n",
    "\n",
    "# Train it\n",
    "model.train(datasetHC)\n",
    "\n",
    "# Evaluate\n",
    "Evaluation.evaluate(datasetPC, model, options, description=f\"{type(datasetHC).__name__}-{options['threshold']}\", epoch=str(options['train']['numEpochs']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XlPoFhpyLgqs"
   },
   "source": [
    "**ceVAE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ujv7TbWVuA5"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "dataset = Dataset.BRAINWEB\n",
    "options = get_options(batchsize=128, learningrate=0.0001, numEpochs=20, zDim=128, outputWidth=128, outputHeight=128, config=get_CONFIG())\n",
    "options['data']['dir'] = options[\"globals\"][dataset.value]\n",
    "datasetHC, datasetPC = get_datasets(options, dataset)\n",
    "config = get_config(trainer=ceVAE, options=options, optimizer='ADAM', intermediateResolutions=[8, 8], dropout_rate=0.1, dataset=datasetHC)\n",
    "\n",
    "config.use_gradient_based_restoration = 0.002\n",
    "\n",
    "# Create an instance of the model and train it\n",
    "model = ceVAE(tf.compat.v1.Session(), config, network=context_encoder_variational_autoencoder.context_encoder_variational_autoencoder)\n",
    "\n",
    "# Train it\n",
    "model.train(datasetHC)\n",
    "\n",
    "# Evaluate\n",
    "Evaluation.evaluate(datasetPC, model, options, description=f\"{type(datasetHC).__name__}-{options['threshold']}\", epoch=str(options['train']['numEpochs']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ruwGvgCnuPQ8"
   },
   "source": [
    "**VAE-Zimmerer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rtNu_izGM8FO"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "dataset = Dataset.BRAINWEB\n",
    "options = get_options(batchsize=64, learningrate=0.0001, numEpochs=20, zDim=128, outputWidth=128, outputHeight=128, config=get_CONFIG())\n",
    "options['data']['dir'] = options[\"globals\"][dataset.value]\n",
    "datasetHC, datasetPC = get_datasets(options, dataset)\n",
    "config = get_config(trainer=VAE, options=options, optimizer='ADAM', intermediateResolutions=[8, 8], dropout_rate=0.1, dataset=datasetHC)\n",
    "\n",
    "# Create an instance of the model and train it\n",
    "model = VAE(tf.compat.v1.Session(), config, network=variational_autoencoder_Zimmerer.variational_autoencoder_Zimmerer)\n",
    "\n",
    "# Train it\n",
    "model.train(datasetHC)\n",
    "\n",
    "# Evaluate\n",
    "Evaluation.evaluate(datasetPC, model, options, description=f\"{type(datasetHC).__name__}-{options['threshold']}\", epoch=str(options['train']['numEpochs']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9lhvpN6mu7QT"
   },
   "source": [
    "**ceVAE-Zimmerer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "szrN4m0eu6xM"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "dataset = Dataset.BRAINWEB\n",
    "options = get_options(batchsize=64, learningrate=0.0001, numEpochs=1, zDim=128, outputWidth=128, outputHeight=128, config=get_CONFIG())\n",
    "options['data']['dir'] = options[\"globals\"][dataset.value]\n",
    "datasetHC, datasetPC = get_datasets(options, dataset)\n",
    "config = get_config(trainer=ceVAE, options=options, optimizer='ADAM', intermediateResolutions=[8, 8], dropout_rate=0.1, dataset=datasetHC)\n",
    "\n",
    "# Create an instance of the model and train it\n",
    "model = ceVAE(tf.compat.v1.Session(), config, network=context_encoder_variational_autoencoder_Zimmerer.context_encoder_variational_autoencoder_Zimmerer)\n",
    "\n",
    "# Train it\n",
    "model.train(datasetHC)\n",
    "\n",
    "# Evaluate\n",
    "Evaluation.evaluate(datasetPC, model, options, description=f\"{type(datasetHC).__name__}-{options['threshold']}\", epoch=str(options['train']['numEpochs']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Oc6ISRcqJMI"
   },
   "source": [
    "#### GMVAE-(Restoration)-Variations\n",
    "\n",
    "Paper: [Unsupervised Lesion Detection via Image Restoration with a Normative Prior](https://openreview.net/forum?id=S1xg4W-leV)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KZmw9-2HO61B"
   },
   "source": [
    "**VAE-You**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VAF3eVrCPAdG"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "dataset = Dataset.BRAINWEB\n",
    "options = get_options(batchsize=128, learningrate=0.0001, numEpochs=20, zDim=128, outputWidth=128, outputHeight=128, config=get_CONFIG())\n",
    "options['data']['dir'] = options[\"globals\"][dataset.value]\n",
    "datasetHC, datasetPC = get_datasets(options, dataset)\n",
    "config = get_config(trainer=VAE_You, options=options, optimizer='ADAM', intermediateResolutions=[8, 8], dropout_rate=0.1, dataset=datasetHC)\n",
    "\n",
    "config.restore_lr = 1e-3\n",
    "config.restore_steps = 10\n",
    "config.tv_lambda = 0.0\n",
    "\n",
    "# Create an instance of the model and train it\n",
    "model = VAE_You(tf.compat.v1.Session(), config, network=variational_autoencoder.variational_autoencoder)\n",
    "\n",
    "# Train it\n",
    "model.train(datasetHC)\n",
    "\n",
    "# Evaluate\n",
    "Evaluation.evaluate(datasetPC, model, options, description=f\"{type(datasetHC).__name__}-{options['threshold']}\", epoch=str(options['train']['numEpochs']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nhllMocWpww9"
   },
   "source": [
    "**GMVAE-You**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TmRGUPN86DTX"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "dataset = Dataset.BRAINWEB\n",
    "options = get_options(batchsize=128, learningrate=0.0001, numEpochs=20, zDim=128, outputWidth=128, outputHeight=128, config=get_CONFIG())\n",
    "options['data']['dir'] = options[\"globals\"][dataset.value]\n",
    "datasetHC, datasetPC = get_datasets(options, dataset)\n",
    "config = get_config(trainer=GMVAE_spatial, options=options, optimizer='ADAM', intermediateResolutions=[8, 8], dropout_rate=0.1, dataset=datasetHC)\n",
    "\n",
    "config.dim_c = 9\n",
    "config.dim_z = 1\n",
    "config.dim_w = 1\n",
    "config.c_lambda = 1\n",
    "config.restore_lr = 1e-3\n",
    "config.restore_steps = 10\n",
    "config.tv_lambda = 1\n",
    "\n",
    "# Create an instance of the model and train it\n",
    "model = GMVAE_spatial(tf.compat.v1.Session(), config, network=gaussian_mixture_variational_autoencoder_You.gaussian_mixture_variational_autoencoder_You)\n",
    "\n",
    "# Train it\n",
    "model.train(datasetHC)\n",
    "\n",
    "# Evaluate\n",
    "Evaluation.evaluate(datasetPC, model, options, description=f\"{type(datasetHC).__name__}-{options['threshold']}\", epoch=str(options['train']['numEpochs']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "azCAeseN3t6i"
   },
   "source": [
    "**GMVAE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o5HNY5v-qCxO"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "dataset = Dataset.BRAINWEB\n",
    "options = get_options(batchsize=128, learningrate=0.0001, numEpochs=20, zDim=128, outputWidth=128, outputHeight=128, config=get_CONFIG())\n",
    "options['data']['dir'] = options[\"globals\"][dataset.value]\n",
    "datasetHC, datasetPC = get_datasets(options, dataset)\n",
    "config = get_config(trainer=GMVAE, options=options, optimizer='ADAM', intermediateResolutions=[8, 8], dropout_rate=0.1, dataset=datasetHC)\n",
    "\n",
    "config.dim_c = 9\n",
    "config.dim_z = 128\n",
    "config.dim_w = 1\n",
    "config.c_lambda = 1\n",
    "config.restore_lr = 1e-3\n",
    "config.restore_steps = 10\n",
    "config.tv_lambda = 0.0\n",
    "\n",
    "# Create an instance of the model and train it\n",
    "model = GMVAE(tf.compat.v1.Session(), config, network=gaussian_mixture_variational_autoencoder.gaussian_mixture_variational_autoencoder)\n",
    "\n",
    "# Train it\n",
    "model.train(datasetHC)\n",
    "\n",
    "# Evaluate\n",
    "Evaluation.evaluate(datasetPC, model, options, description=f\"{type(datasetHC).__name__}-{options['threshold']}\", epoch=str(options['train']['numEpochs']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8_GIxM1KGN2-"
   },
   "source": [
    "**GMVAE-spatial**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m9G0foovGNWI"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "dataset = Dataset.BRAINWEB\n",
    "options = get_options(batchsize=128, learningrate=0.0001, numEpochs=20, zDim=128, outputWidth=128, outputHeight=128, config=get_CONFIG())\n",
    "options['data']['dir'] = options[\"globals\"][dataset.value]\n",
    "datasetHC, datasetPC = get_datasets(options, dataset)\n",
    "config = get_config(trainer=GMVAE_spatial, options=options, optimizer='ADAM', intermediateResolutions=[8, 8], dropout_rate=0.1, dataset=datasetHC)\n",
    "\n",
    "config.dim_c = 9\n",
    "config.dim_z = 1\n",
    "config.dim_w = 1\n",
    "config.c_lambda = 1\n",
    "config.restore_lr = 1e-3\n",
    "config.restore_steps = 10\n",
    "config.tv_lambda = 0.0\n",
    "\n",
    "# Create an instance of the model and train it\n",
    "model = GMVAE_spatial(tf.compat.v1.Session(), config, network=gaussian_mixture_variational_autoencoder_spatial.gaussian_mixture_variational_autoencoder_spatial)\n",
    "\n",
    "# Train it\n",
    "model.train(datasetHC)\n",
    "\n",
    "# Evaluate\n",
    "Evaluation.evaluate(datasetPC, model, options, description=f\"{type(datasetHC).__name__}-{options['threshold']}\", epoch=str(options['train']['numEpochs']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qdIb36bv-yji"
   },
   "source": [
    "#### f-AnoGAN\n",
    "\n",
    "Paper: [f-AnoGAN: Fast unsupervised anomaly detection with generative adversarial networks.](https://www.ncbi.nlm.nih.gov/pubmed/30831356)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B3h_nH3F-0PP"
   },
   "source": [
    "**Unified f-AnoGan**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aj70VsVlAjIj"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "dataset = Dataset.BRAINWEB\n",
    "options = get_options(batchsize=128, learningrate=0.0001, numEpochs=20, zDim=128, outputWidth=128, outputHeight=128, config=get_CONFIG())\n",
    "options['data']['dir'] = options[\"globals\"][dataset.value]\n",
    "datasetHC, datasetPC = get_datasets(options, dataset)\n",
    "config = get_config(trainer=fAnoGAN, options=options, optimizer='ADAM', intermediateResolutions=[8, 8], dropout_rate=0.1, dataset=datasetHC)\n",
    "\n",
    "config.kappa = 1.0\n",
    "config.scale = 10.0\n",
    "\n",
    "# Create an instance of the model and train it\n",
    "model = fAnoGAN(tf.Session(), config, network=fanogan.fanogan)\n",
    "\n",
    "# Train it\n",
    "model.train(datasetHC)\n",
    "\n",
    "# Evaluate\n",
    "Evaluation.evaluate(datasetPC, model, options, description=f\"{type(datasetHC).__name__}-{options['threshold']}\", epoch=str(options['train']['numEpochs']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JP-2nm-jYBuQ"
   },
   "source": [
    "**f-AnoGAN - Schlegl**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hclZeagWA6Rf"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "dataset = Dataset.BRAINWEB\n",
    "options = get_options(batchsize=8, learningrate=0.0001, numEpochs=2, zDim=128, outputWidth=128, outputHeight=128, config=get_CONFIG())\n",
    "options['data']['dir'] = options[\"globals\"][dataset.value]\n",
    "datasetHC, datasetPC = get_datasets(options, dataset)\n",
    "config = get_config(trainer=fAnoGAN, options=options, optimizer='ADAM', intermediateResolutions=[16, 16], dropout_rate=0.1, dataset=datasetHC)\n",
    "\n",
    "config.kappa = 1.0\n",
    "config.scale = 10.0\n",
    "\n",
    "# Create an instance of the model and train it\n",
    "model = fAnoGAN(tf.Session(), config, network=fanogan_schlegl.fanogan_schlegl)\n",
    "\n",
    "# Train it\n",
    "model.train(datasetHC)\n",
    "\n",
    "# Evaluate\n",
    "Evaluation.evaluate(datasetPC, model, options, description=f\"{type(datasetHC).__name__}-{options['threshold']}\", epoch=str(options['train']['numEpochs']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U0D7zuB62DR0"
   },
   "source": [
    "#### Constrained Adversarial AE\n",
    "\n",
    "Paper: [Unsupervised Detection of Lesions in Brain MRI using constrained adversarial auto-encoders](https://arxiv.org/abs/1806.04972)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yh6Tvwe02OWA"
   },
   "source": [
    "**constrained AAE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YPBnr3r32LGf"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "dataset = Dataset.BRAINWEB\n",
    "options = get_options(batchsize=128, learningrate=0.0001, numEpochs=20, zDim=128, outputWidth=128, outputHeight=128, config=get_CONFIG())\n",
    "options['data']['dir'] = options[\"globals\"][dataset.value]\n",
    "datasetHC, datasetPC = get_datasets(options, dataset)\n",
    "config = get_config(trainer=ConstrainedAAE, options=options, optimizer='ADAM', intermediateResolutions=[8, 8], dropout_rate=0.1, dataset=datasetHC)\n",
    "\n",
    "config.scale = 10.0\n",
    "config.rho = 1.0\n",
    "\n",
    "# Create an instance of the model and train it\n",
    "model = ConstrainedAAE(tf.Session(), config, network=constrained_adversarial_autoencoder.constrained_adversarial_autoencoder)\n",
    "\n",
    "# Train it\n",
    "model.train(datasetHC)\n",
    "\n",
    "# Evaluate\n",
    "Evaluation.evaluate(datasetPC, model, options, description=f\"{type(datasetHC).__name__}-{options['threshold']}\", epoch=str(options['train']['numEpochs']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5mR-WjhNv0Mo"
   },
   "source": [
    "**constrained AAE Chen**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m_HhaHSr4Of9"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "dataset = Dataset.BRAINWEB\n",
    "options = get_options(batchsize=8, learningrate=0.0001, numEpochs=2, zDim=128, outputWidth=128, outputHeight=128, config=get_CONFIG())\n",
    "options['data']['dir'] = options[\"globals\"][dataset.value]\n",
    "datasetHC, datasetPC = get_datasets(options, dataset)\n",
    "config = get_config(trainer=ConstrainedAAE, options=options, optimizer='ADAM', intermediateResolutions=[8, 8], dropout_rate=0.1, dataset=datasetHC)\n",
    "\n",
    "config.kappa = 1.0\n",
    "config.scale = 10.0\n",
    "config.rho = 1.0\n",
    "\n",
    "# Create an instance of the model and train it\n",
    "model = ConstrainedAAE(tf.Session(), config, network=constrained_adversarial_autoencoder_Chen.constrained_adversarial_autoencoder_Chen)\n",
    "\n",
    "# Train it\n",
    "model.train(datasetHC)\n",
    "\n",
    "# Evaluate\n",
    "Evaluation.evaluate(datasetPC, model, options, description=f\"{type(datasetHC).__name__}-{options['threshold']}\", epoch=str(options['train']['numEpochs']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_aMjd0DR236b"
   },
   "source": [
    "#### AnoVAEGAN\n",
    "\n",
    "Paper: [Deep autoencoding models for unsupervised anomaly segmentation in brain MR images](https://arxiv.org/abs/1804.04488)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KdHKBg3B2-6W"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "dataset = Dataset.BRAINWEB\n",
    "options = get_options(batchsize=128, learningrate=0.0001, numEpochs=20, zDim=128, outputWidth=128, outputHeight=128, config=get_CONFIG())\n",
    "options['data']['dir'] = options[\"globals\"][dataset.value]\n",
    "datasetHC, datasetPC = get_datasets(options, dataset)\n",
    "config = get_config(trainer=AnoVAEGAN, options=options, optimizer='ADAM', intermediateResolutions=[8, 8], dropout_rate=0.1, dataset=datasetHC)\n",
    "\n",
    "# Create an instance of the model and train it\n",
    "model = AnoVAEGAN(tf.Session(), config, network=anovaegan.anovaegan)\n",
    "\n",
    "# Train it\n",
    "model.train(datasetHC)\n",
    "\n",
    "# Evaluate\n",
    "Evaluation.evaluate(datasetPC, model, options, description=f\"{type(datasetHC).__name__}-{options['threshold']}\", epoch=str(options['train']['numEpochs']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1XhjDLN73NC5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "1-a9c5c4qaiK",
    "P2WmjqCqp3S4",
    "9Oc6ISRcqJMI",
    "qdIb36bv-yji",
    "U0D7zuB62DR0"
   ],
   "machine_shape": "hm",
   "name": "Unsupervised Anomaly Detection Brain-MRI.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
